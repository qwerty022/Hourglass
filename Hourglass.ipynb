{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f98219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import time\n",
    "from torch import optim\n",
    "# import metrics\n",
    "from sklearn import metrics\t\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import cv2\n",
    "# from hlindex import HolisticIndexBlock, DepthwiseO2OIndexBlock, DepthwiseM2OIndexBlock\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import argparse\n",
    "# from carafe import CARAFEPack\n",
    "# from skimage.measure import compare_psnr\n",
    "# from skimage.measure import compare_ssim\n",
    "# from skimage.measure import compare_mse\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from skimage.metrics import mean_squared_error as compare_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96fab790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启用CuDNN（CUDA Deep Neural Network library）加速。\n",
    "cudnn.enabled = True\n",
    "\n",
    "# 添加了三个命令行参数的定义：\n",
    "# 1--upmode 用于指定实现上采样的模式。默认值是'nn'，表示使用最近邻插值法进行上采样\n",
    "# 2--description 用于指定描述信息的路径 表示描述信息的路径为当前目录下的'0130'文件夹。\n",
    "# 3--evaluate 用于指定模型的路径 表示模型路径为'experiments/0130/nn/model_best.pth.tar'\n",
    "# 解析命令行参数，并返回一个包含解析结果的命名空间对象。\n",
    "def get_arguments():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Hourglass\")\n",
    "\n",
    "    parser.add_argument(\"--upmode\", type=str, default='nn', help=\"the mode chosen to implement upsample.\") # 'bilinear', 'maxpool', 'nn'. 'indexnet-m2o', ...\n",
    "    parser.add_argument(\"--description\", type=str, default='./0701/', help=\"description.\")\n",
    "    parser.add_argument(\"--evaluate\", type=str, default='experiments/0701/nn/model_best.pth.tar', help=\"path of model.\")\n",
    "\n",
    "    #return parser.parse_args()\n",
    "    return parser.parse_args(args=[])\n",
    "\n",
    "# 输入参数为x和block_size，其中x是一个张量（tensor），block_size是一个整数，表示块的大小。\n",
    "# 其中n表示批次大小（batch size），c表示通道数（channel），h表示高度（height），w表示宽度（width）。\n",
    "# 使用F.unfold()函数对输入张量x进行展开操作，展开的块大小为block_size，步长为block_size。这将把输入张量的每个块展开为一列，并按照展开的顺序排列。\n",
    "# 使用unfolded_x.view()函数对展开后的张量进行形状变换，将其重新变为原始形状。\n",
    "# 其中，n保持不变，c乘以block_size的平方，表示每个块的展开后的通道数，h和w分别除以block_size，表示每个块的展开后的高度和宽度。\n",
    "# 返回变换后的张量。\n",
    "def space_to_depth(x, block_size):\n",
    "    n, c, h, w = x.size()\n",
    "    unfolded_x = F.unfold(x, block_size, stride=block_size)\n",
    "    return unfolded_x.view(n, c*block_size**2, h//block_size, w//block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1191f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBNReLU(in_channels,out_channels,kernel_size,stride,padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=kernel_size,stride=stride,padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        mid_channels = out_channels//2\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ConvBNReLU(in_channels=in_channels, out_channels=mid_channels, kernel_size=1, stride=1),\n",
    "            ConvBNReLU(in_channels=mid_channels, out_channels=mid_channels, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBNReLU(in_channels=mid_channels, out_channels=out_channels, kernel_size=1, stride=1),\n",
    "        )\n",
    "        self.shortcut = ConvBNReLU(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bottleneck(x)\n",
    "        return out+self.shortcut(x)\n",
    "\n",
    "\n",
    "class HourglassModule(nn.Module):\n",
    "    def __init__(self, nChannels=256, nModules=2, numReductions = 4):\n",
    "        super(HourglassModule, self).__init__()\n",
    "        self.nChannels = nChannels\n",
    "        self.nModules = nModules\n",
    "        self.numReductions = numReductions\n",
    "\n",
    "        self.residual_block = self._make_residual_layer(self.nModules, self.nChannels)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.after_pool_block = self._make_residual_layer(self.nModules, self.nChannels)\n",
    "\n",
    "        if numReductions > 1:\n",
    "            self.hourglass_module = HourglassModule(self.nChannels, self.numReductions - 1, self.nModules)\n",
    "        else:\n",
    "            self.num1res_block = self._make_residual_layer(self.nModules, self.nChannels)\n",
    "\n",
    "        self.lowres_block = self._make_residual_layer(self.nModules, self.nChannels)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "    def _make_residual_layer(self, nModules, nChannels):\n",
    "        _residual_blocks = []\n",
    "        for _ in range(nModules):\n",
    "            _residual_blocks.append(ResidualBlock(in_channels=nChannels, out_channels=nChannels))\n",
    "        return nn.Sequential(*_residual_blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.residual_block(x)\n",
    "\n",
    "        out2 = self.max_pool(x)\n",
    "        out2 = self.after_pool_block(out2)\n",
    "\n",
    "        if self.numReductions > 1:\n",
    "            out2 = self.hourglass_module(out2)\n",
    "        else:\n",
    "            out2 = self.num1res_block(out2)\n",
    "        out2 = self.lowres_block(out2)\n",
    "        out2 = self.upsample(out2)\n",
    "\n",
    "        return out1 + out2\n",
    "\n",
    "class Hourglass(nn.Module):\n",
    "    def __init__(self, nJoints):\n",
    "        super(Hourglass, self).__init__()\n",
    "\n",
    "        self.first_conv = ConvBNReLU(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
    "        self.residual_block1 = ResidualBlock(in_channels=64,  out_channels=128)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.residual_block2 = ResidualBlock(in_channels=128, out_channels=128)\n",
    "        self.residual_block3 = ResidualBlock(in_channels=128, out_channels=256)\n",
    "\n",
    "        self.hourglass_module1 = HourglassModule(nChannels=256, nModules=1, numReductions = 4)\n",
    "        self.hourglass_module2 = HourglassModule(nChannels=256, nModules=1, numReductions = 4)\n",
    "\n",
    "        self.after_hourglass_conv1 = ConvBNReLU(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.proj_conv1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, stride=1)\n",
    "        self.out_conv1 = nn.Conv2d(in_channels=256,out_channels=nJoints,kernel_size=1,stride=1)\n",
    "        self.remap_conv1 = nn.Conv2d(in_channels=nJoints, out_channels=256, kernel_size=1, stride=1)\n",
    "\n",
    "        self.after_hourglass_conv2 = ConvBNReLU(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.proj_conv2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, stride=1)\n",
    "        self.out_conv2 = nn.Conv2d(in_channels=256, out_channels=nJoints, kernel_size=1, stride=1)\n",
    "        self.remap_conv2 = nn.Conv2d(in_channels=nJoints, out_channels=256, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(self.residual_block1(self.first_conv(x)))\n",
    "        x = self.residual_block3(self.residual_block2(x))\n",
    "\n",
    "        x = self.hourglass_module1(x)\n",
    "        residual1= x = self.after_hourglass_conv1(x)\n",
    "        out1 = self.out_conv1(x)\n",
    "        residual2 =  x = residual1 + self.remap_conv1(out1)+self.proj_conv1(x)\n",
    "\n",
    "        x = self.hourglass_module2(x)\n",
    "        x = self.after_hourglass_conv2(x)\n",
    "        out2 = self.out_conv2(x)\n",
    "        x = residual2 + self.remap_conv2(out2) + self.proj_conv2(x)\n",
    "       \n",
    "        return torch.nn.functional.interpolate(out1 + out2, scale_factor=4, mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    running_loss = 0\t# 用于记录每个epoch的累计损失\n",
    "    model.train()\t\t# 将模型设置为训练模式\n",
    "    cudnn.benchmark = True\n",
    "    # 迭代训练数据加载器中的批次数据\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        #if i == 20:\n",
    "        #   break;\n",
    "        optimizer.zero_grad()\t# 将优化器的梯度缓存清零\n",
    "        output = model(data)\t# 通过模型前向传播得到输出\n",
    "        target = data.clone()\t# 克隆数据作为目标\n",
    "        target = target * 0.3081 + 0.1307\t# 对目标数据进行标准化处理\n",
    "        loss = criterion(output, target)\t# 计算损失值\n",
    "        loss.backward()\t\t# 进行反向传播计算梯度\n",
    "        optimizer.step()\t# 更新模型的参数\n",
    "        running_loss += loss.item()\t\t# 累计当前批次的损失值\n",
    "        #print(\"train\")\n",
    "        #print(i)\n",
    "    # 将每个epoch的平均损失值存储在模型的train_loss字典中的epoch_loss列表中\n",
    "    model.train_loss['epoch_loss'].append(running_loss / (i + 1))\t\n",
    "\n",
    "\n",
    "# calculate loss\n",
    "def val(epoch, model, val_loader, criterion, save_image_dir):\n",
    "    model.eval()\t# 评估模式\n",
    "    cudnn.benchmark = False\t\t# 并禁用cudnn.benchmark以节省内存\n",
    "    val_loss = 0\n",
    "    psnr = []\n",
    "    ssim = []\n",
    "    mse = []\n",
    "    mae = []\n",
    "    # 创建一个保存当前epoch图像的目录\n",
    "    save_image_dir_epoch = save_image_dir + 'epoch{}/'.format(epoch)\n",
    "    if not os.path.exists(save_image_dir_epoch):\n",
    "        os.makedirs(save_image_dir_epoch)\n",
    "    # 使用torch.no_grad()上下文管理器，禁用梯度计算。\n",
    "    # 在验证数据加载器中迭代批次数据，每个批次的数据移动到GPU上\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(val_loader):\n",
    "            data = data.cuda()\n",
    "            output = model(data)\t# 通过模型前向传播得到输出\n",
    "            target = data.clone()\t# 克隆数据作为目标\n",
    "            target = target * 0.3081 + 0.1307\t# 进行标准化处理\n",
    "            val_loss += criterion(output, target).item() # sum up batch loss\n",
    "            # 计算验证损失值,并将其累加到val_loss中\n",
    "\n",
    "            # PSNR（峰值信噪比）\n",
    "            # SSIM（结构相似度指数）\n",
    "            # MSE（均方误差）\n",
    "            # MAE（平均绝对误差）\n",
    "            # psnr.append(psnr_value)用于将计算得到的psnr_value添加到psnr列表的末尾\n",
    "            # squeeze()去除维度为1的维度,cpu()将张量从GPU内存中移动到CPU内存\n",
    "            # numpy()将PyTorch张量转换为NumPy数组的方法\n",
    "            # astype(np.float32)将数组的数据类型转换为指定的数据类型单精度浮点型\n",
    "            # 将PyTorch张量经过挤压、从GPU移回CPU，并转换为NumPy数组，并且将其数据类型转换为单精度浮点型\n",
    "            # 为data_range参数传递图像数据的动态范围。通常情况下，取值范围为图像数据类型的最大值减去最小值。\n",
    "            psnr.append(compare_psnr(target.squeeze().cpu().numpy().astype(np.float32), output.squeeze().cpu().numpy().astype(np.float32)))\n",
    "            ssim.append(compare_ssim(target.squeeze().cpu().numpy().astype(np.float32), output.squeeze().cpu().numpy().astype(np.float32),data_range=32))\n",
    "            mse.append(np.sqrt(compare_mse(target.squeeze().cpu().numpy().astype(np.float32), output.squeeze().cpu().numpy().astype(np.float32))))\n",
    "            mae.append(criterion(output, target).item())\n",
    "            #if i == 20:\n",
    "            #   break;\n",
    "            # 如果需要保存图像，可以取消代码片段中的注释，并将输出和目标图像保存到指定的目录中。\n",
    "\n",
    "            if i < 20:\n",
    "                outputs_save = np.clip(output[0].cpu().numpy(), 0, 1) * 255\n",
    "                target_save = np.clip(target[0].cpu().numpy(), 0, 1) * 255\n",
    "                cv2.imwrite(save_image_dir_epoch + 'epoch{}_{}.jpg'.format(epoch, i), outputs_save.squeeze().astype(np.uint8))\n",
    "                cv2.imwrite(save_image_dir_epoch + 'epoch{}_{}_gt.jpg'.format(epoch, i), target_save.squeeze().astype(np.uint8))\n",
    "\n",
    "    val_loss /= (i + 1)\n",
    "    # 将计算得到的验证损失和评估指标存储在模型的val_loss字典和measure字典中的相应列表中。\n",
    "    model.val_loss['epoch_loss'].append(val_loss)\n",
    "    model.measure['psnr'].append(np.mean(psnr))\n",
    "    model.measure['ssim'].append(np.mean(ssim))\n",
    "    model.measure['mse'].append(np.mean(mse))\n",
    "    model.measure['mae'].append(np.mean(mae))\n",
    "\n",
    "\n",
    "def save_checkpoint(state, snapshot_dir, filename='.pth.tar'):\n",
    "    torch.save(state, '{}/{}'.format(snapshot_dir, filename))\n",
    "\n",
    "def main():\n",
    "    batchsize = 100\n",
    "    test_batchsize = 80\n",
    "    epoches = 100\n",
    "    args = get_arguments()\t# 获取命令行参数\n",
    "    image_size = 32\n",
    "\n",
    "    # 标准化操作使用了均值(0.1307,)和标准差(0.3081,)进行归一化处理。\n",
    "    transform = transforms.Compose([transforms.Resize(image_size),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,)), ])\n",
    "    # train=True表示创建训练集\n",
    "    trainset = torchvision.datasets.FashionMNIST(root='./data/fashion', train=True, download=True,transform=transform)\n",
    "    # batch_size参数指定每个批次的样本数量，shuffle=True表示在每个周期开始前对数据进行洗牌，num_workers参数指定数据加载的线程数。\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, shuffle=True, num_workers=2)\n",
    "    # train=False表示创建测试集，\n",
    "    testset = torchvision.datasets.FashionMNIST(root='./data/fashion', train=False, download=True,transform=transform)\n",
    "    # batch_size参数指定每个批次的样本数量，shuffle=False表示不对数据进行洗牌，num_workers参数指定数据加载的线程数。\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=test_batchsize, shuffle=False, num_workers=2)\n",
    "\n",
    "    # save info of model\n",
    "    trained_model_dir = 'experiments/' + args.description + '{}/'.format(args.upmode)\n",
    "    train_info_record = trained_model_dir + args.upmode + '.txt'\n",
    "    save_image_dir = trained_model_dir + 'results/'\n",
    "\n",
    "    if not os.path.exists(trained_model_dir):\n",
    "        os.makedirs(trained_model_dir)\n",
    "\n",
    "    if not os.path.exists(save_image_dir):\n",
    "        os.makedirs(save_image_dir)\n",
    "    # 升采样模式\n",
    "    net = Hourglass(1)\n",
    "    net.cuda()\n",
    "    # 使用了SGD优化器,指定学习率lr=0.01和动量momentum=0.9。\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "    # 创建损失函数对象,使用了L1损失函数\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    net.train_loss = {\n",
    "        'epoch_loss': []\n",
    "    }\n",
    "    net.val_loss = {\n",
    "        'epoch_loss': []\n",
    "    }\n",
    "    net.measure = {\n",
    "        'psnr': [],\n",
    "        'ssim': [],\n",
    "        'mse': [],\n",
    "        'mae': []\n",
    "    }\n",
    "\n",
    "    if os.path.isfile(args.evaluate):\n",
    "        checkpoint = torch.load(args.evaluate)\n",
    "        net.load_state_dict(checkpoint['state_dict'])\t# 将加载的模型参数加载到网络模型net中。\n",
    "        epoch = checkpoint['epoch']\n",
    "        val(epoch, net, testloader, criterion, save_image_dir)\n",
    "        print(' sample: %d psnr: %.2f%%  ssim: %.4f  mse: %0.6f mae: %0.6f' % (\n",
    "            len(testloader.dataset), net.measure['psnr'][-1], net.measure['ssim'][-1], net.measure['mse'][-1], net.measure['mae'][-1]))\n",
    "        return\n",
    "\n",
    "    # train begin\n",
    "    print('training begin')\n",
    "    # 调度器来设置学习率的调整策略\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[50, 70, 85], gamma=0.1)\n",
    "    for epoch in range(epoches):\n",
    "        scheduler.step()\t# 更新学习率\n",
    "        start = time.time()\n",
    "        train(net, trainloader, optimizer, criterion)\n",
    "        end = time.time()\n",
    "        print('epoch: %d sample: %d cost %.5f seconds  loss: %.5f' % (\n",
    "        epoch, len(trainloader.dataset), (end - start), net.train_loss['epoch_loss'][-1]))\n",
    "\n",
    "        val(epoch, net, testloader, criterion, save_image_dir)\n",
    "        print(' sample: %d test_loss: %.5f psnr: %.2f%%  ssim: %.4f' % (\n",
    "        len(testloader.dataset), net.val_loss['epoch_loss'][-1], net.measure['psnr'][-1], net.measure['ssim'][-1]))\n",
    "        # save checkpoint\n",
    "        state = {\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': net.train_loss,\n",
    "            'val_loss': net.val_loss,\n",
    "            'measure': net.measure\n",
    "        }\n",
    "        # save model\n",
    "        save_checkpoint(state, trained_model_dir, filename='model_ckpt.pth.tar')\n",
    "        # 如果当前轮数的PSNR指标比之前的轮数都要好,保存最佳模型\n",
    "        if len(net.measure['psnr']) > 1 and net.measure['psnr'][-1] >= max(net.measure['psnr'][:-1]):\n",
    "            save_checkpoint(state, trained_model_dir, filename='model_best.pth.tar')\n",
    "\n",
    "        with open(train_info_record, 'a') as f:\n",
    "            f.write(\n",
    "                'lr:{}, epoch:{}, train_loss:{:.4f}, val_loss:{:.6f}, psnr:{:.2f}, ssim:{:.2f}'.format(\n",
    "                    optimizer.param_groups[0]['lr'], epoch, net.train_loss['epoch_loss'][-1],\n",
    "                    net.val_loss['epoch_loss'][-1], net.measure['psnr'][-1], net.measure['ssim'][-1]) + '\\n'\n",
    "            )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
